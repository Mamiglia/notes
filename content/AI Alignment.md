---
title: AI Alignment
description: 
date: 13/01/2025
tags:
  - notes
url:
---
**Alignment:** making AI systems try to do what their creators intend them to do (some people call this _intent alignment_).
**Capabilities:** developing AI systems that effectively carry out the tasks that they are trying to achieve.

These two properties are often in contrast and a competitive AI system needs to show both of them in order to be properly useful. Itâ€™s also often very difficult to distinguish competence and alignment failures: it can be unclear what AI models are actually capable of, and whether failures represent:
- inability to do tasks correctly (competence); or
- that the model can do them but is trying to do something else (alignment)

## Outer VS Inner Misalignment
![aa](https://bluedot.org/u/2024/03/inner-outer-alignment-person-diagram-2.png)

Misalignment problems is usually broke down in two subtasks:
- **Outer Misalignment** refers to the complexity of expressing an ideal goal into a defined set of precise rules.
- **Inner Misalignment** refers instead to the gaming that an AI system could apply to a given proxy goal distill an internal goal.


